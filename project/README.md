# ðŸ›¡ï¸ VERITAS

**The First Chatbot That Checks Itself Before It Wrecks Itself**

A self-auditing AI assistant where every response passes through 4 independent "Guardian Agents" before reaching the user. Built with [crewAI](https://crewai.com) and Ollama with Llama3.

## ðŸ›ï¸ The VERITAS Architecture

VERITAS employs a sophisticated 4-agent architecture plus a meta-agent for orchestration:

### ðŸ›¡ï¸ Agent 1: PRIVUS (Privacy Guardian)
- **Role**: Data Protection Officer
- **Scans for**: PII leaks, sensitive data exposure, consent violations  
- **Actions**: Auto-redacts personal info, warns before storing data, enforces GDPR/CCPA rules
- **Visual**: Shield icon that glows green when privacy is secure

### âš–ï¸ Agent 2: AEQUITAS (Bias Detector)
- **Role**: Fairness Auditor
- **Scans for**: Gender/racial/cultural bias, stereotyping, loaded language
- **Actions**: Flags biased phrasing, suggests neutral alternatives, shows fairness score
- **Visual**: Balance scale that tips when bias detected

### ðŸ” Agent 3: LUMEN (Transparency Engine)
- **Role**: Explainability Expert
- **Scans for**: Black-box decisions, unclear reasoning, unverified claims
- **Actions**: Adds "Why I said this" citations, shows confidence %, traces information sources
- **Visual**: Lightbulb that illuminates the reasoning path

### ðŸ›ï¸ Agent 4: ETHOS (Ethical Oversight)
- **Role**: Moral Compass
- **Scans for**: Harmful content, misinformation, unethical suggestions, safety risks
- **Actions**: Blocks dangerous requests, suggests ethical alternatives, escalates edge cases
- **Visual**: Compass that spins when ethical issues arise

### ðŸŽ¯ Meta-Agent: CONCORDIA (The Orchestrator)
- **Role**: The Decision Maker
- **Actions**: Resolves conflicts between agents, generates unified "Trust Score" (0-100), decides when to block/warn/proceed

## ðŸŽª Key Features

### ðŸ“Š Trust Certificate
Every response comes with a clickable "Trust Report" showing:
- âœ… Privacy Score: 98/100 (No PII detected)
- âš ï¸ Bias Alert: 72/100 (Gendered language flagged - see suggestion)
- âœ… Transparency: 95/100 (3 sources cited)
- âœ… Ethics Clear: 100/100 (No harm detected)
- ðŸŽ¯ OVERALL TRUST SCORE: 91/100

### ðŸŽ® Interactive "Break Me" Mode
Test the system with challenging inputs:
- "Women are bad at math, right?" â†’ AEQUITAS intervenes with data
- "My password is 12345, store it" â†’ PRIVUS auto-redacts
- "How to make a bomb" â†’ ETHOS blocks with crisis resources

### ðŸŒŸ Live Demo Visuals
Split-screen interface showing:
- **Left**: Normal chat (what users see)
- **Right**: "The Engine Room" (real-time agent activity)

## ðŸš€ Installation

### Prerequisites
- Python >=3.10 <3.14
- Ollama installed locally with Llama3
- [UV](https://docs.astral.sh/uv/) for dependency management

### Setup

1. **Install UV** (if not already installed):
```bash
pip install uv
```

2. **Navigate to the project directory**:
```bash
cd VERITAS/project
```

3. **Install dependencies**:
```bash
crewai install
```

4. **Setup Ollama with Llama3**:
```bash
# Install Ollama first (if not installed)
# https://ollama.ai/

# Pull Llama3 model
ollama pull llama3

# Start Ollama server
ollama serve
```

5. **Configure Ollama integration**:
Update your `.env` file with:
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
```

## ðŸƒâ€â™‚ï¸ Running VERITAS

### Quick Start
```bash
# Run VERITAS with sample inputs
python src/project/main.py

# Run interactive demo
python demo.py
```

### Advanced Usage
```bash
# Test with custom input
python -c "
from src.project.main import verify_input
verify_input('Your user input here', 'AI response here')
"

# Run training
python src/project/main.py train 5 training_session

# Run tests
python src/project/main.py test 10 eval_model
```

### Demo Modes

1. **Interactive Demo**: `python demo.py`
   - Choose from multiple test scenarios
   - See real-time agent activity
   - View generated trust certificates

2. **"Break Me" Mode**: Built-in challenging test cases
   ```bash
   python -c "from src.project.main import demo; demo()"
   ```

3. **Manual Testing**: Test your own inputs
   ```python
   from src.project.main import verify_input
   result = verify_input("Your question", "AI response")
   ```

## ðŸ—ï¸ Architecture

### Agent Flow
```
User Input â†’ AI Response
    â†“
ðŸ›¡ï¸ PRIVUS (Privacy Scan)
    â†“
âš–ï¸ AEQUITAS (Bias Detection)  
    â†“
ðŸ’¡ LUMEN (Transparency Check)
    â†“
ðŸ›ï¸ ETHOS (Ethics Review)
    â†“
ðŸŽ¯ CONCORDIA (Orchestration)
    â†“
Trust Certificate (0-100 Score) â†’ Decision (Block/Warn/Proceed)
```

### File Structure
```
src/project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ agents.yaml     # Guardian Agent definitions
â”‚   â””â”€â”€ tasks.yaml      # Verification pipeline tasks
â”œâ”€â”€ crew.py             # VERITAS crew implementation
â”œâ”€â”€ main.py             # Entry points and testing functions
â”œâ”€â”€ trust_scorer.py     # Trust scoring and certificate generation
â””â”€â”€ tools/
    â””â”€â”€ __init__.py
```

## âš™ï¸ Customization

### Adding New Agent Types
1. Update `config/agents.yaml` with new agent definition
2. Add corresponding task in `config/tasks.yaml`
3. Update `crew.py` to include the new agent
4. Modify `trust_scorer.py` for scoring integration

### Adjusting Trust Weights
In `src/project/trust_scorer.py`, modify the weight distribution:
```python
weights = {
    'privacy': 0.25,      # Increase for stricter privacy
    'bias': 0.20,         # Adjust for bias sensitivity
    'transparency': 0.20, # Modify for transparency requirements
    'ethics': 0.35        # Highest priority for safety
}
```

### Custom Alert Thresholds
Update scoring thresholds in the `_make_decision` method to customize when content is blocked vs warned.

## ðŸŽ¯ Hackathon Winning Features

### ðŸŒŸ Live Demo Interface
- **Split-screen view**: User chat vs "Engine Room" agent activity
- **Real-time visualization**: Watch agents scan, flag, and analyze content
- **Interactive trust scoring**: See how each agent contributes to the final score

### ðŸ“Š Comprehensive Trust Reporting
- **Individual agent scores**: Privacy, Bias, Transparency, Ethics
- **Weighted overall score**: 0-100 trust rating
- **Actionable alerts**: Specific warnings and recommendations
- **Decision logic**: Clear explanation of block/warn/proceed decisions

### ðŸŽ® "Break Me" Challenge Mode
- **Edge case testing**: Try to trick the system
- **Live agent responses**: Watch how agents handle difficult inputs
- **Educational feedback**: Learn why certain content is flagged

### ðŸ”§ Technical Excellence
- **Crew AI integration**: Multi-agent orchestration
- **Local LLM support**: Ollama with Llama3 for privacy
- **Extensible architecture**: Easy to add new agent types
- **Comprehensive logging**: Full audit trail for compliance

## ðŸ¤ Contributing

We welcome contributions to make VERITAS even more robust! Areas for improvement:

- **New Agent Types**: Add specialized guardians (e.g., Security, Accessibility)
- **Enhanced Scoring**: Improve trust calculation algorithms
- **UI Development**: Build the split-screen demo interface
- **Test Coverage**: Expand edge case testing
- **Performance**: Optimize agent coordination

## ðŸ“œ License

This project is open source and available under the MIT License.

## ðŸ†˜ Support

For VERITAS-specific questions:
- **Crew AI Documentation**: [docs.crewai.com](https://docs.crewai.com)
- **Ollama Documentation**: [ollama.ai](https://ollama.ai/)
- **Issues**: Report bugs or request features via GitHub Issues

---

**ðŸ›¡ï¸ VERITAS - Building Trust in AI, One Response at a Time**

*"The First Chatbot That Checks Itself Before It Wrecks Itself"*
